{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a77807f92f26ee",
   "metadata": {},
   "source": [
    "# DiCoW: Diarization-Conditioned Whisper for Target Speaker ASR â€“ JSALT 25 Competition\n",
    "\n",
    "> ğŸ¯ *Were you wondering if training on the eval set is allowed?*\n",
    "> **Nope, itâ€™s not.** âŒ\n",
    "\n",
    "Be sure to change the runtime type to **GPU** in the menu above:\n",
    "**Runtime -> Change runtime type -> Hardware accelerator -> GPU!!!**\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ† Competition Information\n",
    "\n",
    "Welcome to the **DiCoW Target Speaker ASR Challenge!**\n",
    "\n",
    "**Prizes** *(valid for one glorious evening only)*:\n",
    "\n",
    "* ğŸ¥‡ **1st Place:** ğŸº Infinite beers\\* â€” until you start explaining quantum physics to a plant ğŸŒ±ğŸ¤¯ğŸ»\n",
    "  *(â€œInfiniteâ€ ends when you try to high-five the floor.)*\n",
    "\n",
    "* ğŸ¥ˆ **2nd Place:** ğŸºğŸºğŸº 3 beers â€” just enough to convince yourself you can do parkour over chairs ğŸª‘ğŸ§—â€â™‚ï¸\n",
    "\n",
    "* ğŸ¥‰ **3rd Place:** ğŸº 1 beer â€” sip it like itâ€™s liquid gold ğŸ…ğŸºâœ¨\n",
    "\n",
    "---\n",
    "\n",
    "**Submission:** Submit your best performing systems to the [EMMA Leaderboard](https://huggingface.co/spaces/BUT-FIT/EMMA_leaderboard).\n",
    "\n",
    "**Deadline:** ğŸ—“ï¸ *June 17th, 2025 at 16:00 CET*\n",
    "\n",
    "**SUBMISSION\\_TOKEN:** `emmA25`\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ› ï¸ Challenge Tasks\n",
    "1. Clone the DiCoW repository and set up the environment\n",
    "2. Prepare the Libri2Mix dataset\n",
    "3. Finetune Whisper tiny model using DiCoW\n",
    "4. Evaluate on Libri2Mix clean test set\n",
    "5. Submit results to EMMA leaderboard\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“š Table of Contents\n",
    "1. [Introduction to MT-ASR, TS-ASR, and DiCoW](#intro)\n",
    "    1. [Multi-Talker ASR (MT-ASR)](#mt_asr)\n",
    "    2. [Target Speaker ASR (TS-ASR)](#ts_asr)\n",
    "    3. [DiCoW: Diarization-Conditioned Whisper](#dicow)\n",
    "        3. [Preprocessing & EDA](#Preprocessing--EDA)\n",
    "2. [Environment Setup](#setup)\n",
    "3. [Data Preparation](#data)\n",
    "4. [Model Training](#training)\n",
    "5. [Decoding & Evaluation](#decoding)\n",
    "6. [Submission Guidelines](#submission)\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“– Resources\n",
    "1. Repositories:\n",
    "    - [DiCoW GitHub Repository](https://github.com/BUTSpeechFIT/TS-ASR-Whisper)\n",
    "    - [DiCoW Inference Repository](https://github.com/BUTSpeechFIT/DiCoW)\n",
    "2. Papers:\n",
    "    - [Target Speaker ASR with Whisper](https://ieeexplore.ieee.org/document/10887683)\n",
    "    - [DiCoW: Diarization-Conditioned Whisper for Target Speaker ASR](https://arxiv.org/abs/2501.00114)\n",
    "    - [BUT/JHU System Description for CHiME-8 NOTSOFAR-1 Challenge](https://www.isca-archive.org/chime_2024/polok24_chime.html)\n",
    "    - BUT System for the MLC-SLM Challenge\n",
    "3. Demo:\n",
    "    - [DiCoW Gradio Demo](https://pccnect.fit.vutbr.cz/gradio-demo/)\n",
    "4. Leaderboard:\n",
    "    - [EMMA Leaderboard](https://huggingface.co/spaces/BUT-FIT/EMMA_leaderboard)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9403faf6ea69280d",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. INTRODUCTION TO MULTI-TALKER AUTOMATIC SPEECH RECOGNITION\n",
    "\n",
    "\n",
    "![mt_asr](img/mt_asr.png)\n",
    "\n",
    "### The Challenge of Multi-Talker ASR\n",
    "\n",
    "Automatic Speech Recognition (ASR) systems traditionally work well with single-speaker audio.\n",
    "However, real-world scenarios often involve multiple speakers talking simultaneously, creating\n",
    "several challenges:\n",
    "\n",
    "1. **Overlapping Speech**: Multiple speakers talking at the same time\n",
    "2. **Speaker Confusion**: Difficulty determining who said what\n",
    "3. **Acoustic Interference**: Speech from one speaker masks another\n",
    "4. **Variable Number of Speakers**: Unknown number of active speakers\n",
    "\n",
    "### Approaches to Multi-Talker ASR\n",
    "\n",
    "![mt_asr_approaches](img/mt_asr_approaches.png)\n",
    "\n",
    "\n",
    "1. **Speech Separation + ASR**: First separate speakers, then apply ASR\n",
    "2. **E2E MT-ASR (SOT)**: Concatenate speaker-attributed transcriptions by emission time\n",
    "3. **Target Speaker ASR**: Focus on specific speaker of interest\n",
    "\n",
    "### Metrics for Multi-Talker ASR\n",
    "1. Optimal Reference Combination Word Error Rate (ORC WER)\n",
    "2. Concatenated minimum-Permutation Word Error Rate (cpWER)\n",
    "3. Time-Constrained minimum-Permutation Word Error Rate (tcpWER)\n",
    "\n",
    "https://github.com/fgnt/meeteval\n",
    "\n",
    "https://groups.uni-paderborn.de/nt/meeteval/icassp2024-demo/poster_example.html?selection=28.6-38.7&minimaps=1\n",
    "\n",
    "### Target Speaker ASR: Focus on Speaker of Interest\n",
    "\n",
    "Target Speaker ASR (TS-ASR) addresses a practical scenario: given mixed audio with multiple\n",
    "speakers, transcribe only the speech from a specific target speaker.\n",
    "\n",
    "\n",
    "### Traditional TS-ASR Approaches:\n",
    "\n",
    "1. Randomly initialized model and i-vector based speaker embeddings\n",
    "\n",
    "![ts_asr_embed](img/ts_asr_embed.png)\n",
    "\n",
    "2. Pretrained ASR model with (better) speaker embeddings\n",
    "\n",
    "![ts_asr_enrolment](img/ts_asr_enrolment.png)\n",
    "\n",
    "3. Pretrained ASR model directly conditioned on speaker enrolment\n",
    "\n",
    "![whisper_enrolment](img/whisper_enrolment.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12bfa19f046f163c",
   "metadata": {},
   "source": [
    "### DICOW: DIARIZATION-CONDITIONED WHISPER\n",
    "\n",
    "DiCoW (Diarization-Conditioned Whisper) represents a paradigm shift in Target Speaker ASR.\n",
    "Instead of relying on speaker embeddings, DiCoW leverages speaker diarization outputs as\n",
    "conditioning information.\n",
    "\n",
    "![dicow](img/dicow.png)\n",
    "\n",
    "#### Advantages of DiCoW\n",
    "\n",
    "1. **No Speaker Embeddings Required**: Eliminates dependency on embedding quality\n",
    "2. **Better Generalization**: Works well with unseen speakers\n",
    "3. **Simplified Workflow**: Direct conditioning on diarization outputs\n",
    "4. **Maintains Whisper Performance**: Preserves accuracy on (multi-lingual) single-speaker data\n",
    "\n",
    "#### STNO - Silence, Target, Non-Target, and Overlap Masks\n",
    "\n",
    "Let $\\mathbf{D} \\in [0,1]^{S \\times T}$, where $S$ is the number of speakers in the recording, and $T$ is the number of frames, represent the diarization output, with $d(s, t)$ denoting the probability that speaker $s$ is active in time frame $t$. Let $s_k$ represent the target speaker.\n",
    "We define a distribution over the following mutually exclusive events for a frame at time $t$:\n",
    "\n",
    "1. ${\\mathcal{S}}$: Time frame $t$ represents silence.\n",
    "2. ${\\mathcal{T}}$: The target speaker, $s_k$, is the only active speaker in time frame $t$.\n",
    "3. ${\\mathcal{N}}$: One or more non-target speakers, $s \\neq s_k$ are active and the target speaker, $s_k$, is not active at time frame $t$.\n",
    "4. ${\\mathcal{O}}$: The target speaker $s_k$ is active while at least one non-target speaker $s \\neq s_k$ is also active at time frame $t$, denoting an overlap.\n",
    "\n",
    "\n",
    "The probabilities of these events occurring at time frame $t$ can be calculated as:\n",
    "1. $p_{\\mathcal{S}}^t  = \\prod_{s=1}^S (1 - d(s, t))$\n",
    "2. $p_{\\mathcal{T}}^t  = d(s_k, t)  \\prod_{\\substack{s=1 \\\\ s \\neq s_k}}^S (1 - d(s, t))$\n",
    "3. $p_{\\mathcal{N}}^t  = \\left(1 - p_{\\mathcal{S}}^t\\right) - d\\left(s_k, t\\right)$\n",
    "4. $p_{\\mathcal{O}}^t  = d(s_k, t) - p_{\\mathcal{T}}^t$\n",
    "\n",
    "\n",
    "This definition allows us to use a fixed-sized STNO (Silence, Target, Non-target, Overlap) mask $\\mathbf{M}^t = \\begin{bmatrix} p_{\\mathcal{S}}^t & p_{\\mathcal{T}}^t & p_{\\mathcal{N}}^t & p_{\\mathcal{O}}^t \\end{bmatrix}^{\\top}$.\n",
    "\n",
    "### #Frame-Level Diarization Dependent Transformations\n",
    "\n",
    "\n",
    "Let $\\mathbf{Z}^l \\in \\mathbb{R}^{d_{{m}} \\times T}$ represent the frame-by-frame inputs to the $l$-th (Transformer) layer.\n",
    "\n",
    "We transform these hidden representations by applying four affine STNO layer- and class-specific transformations: $\\mathbf{W}_{\\mathcal{S}}^l, \\mathbf{W}_{\\mathcal{T}}^l, \\mathbf{W}_{\\mathcal{N}}^l, \\mathbf{W}_{\\mathcal{O}}^l \\in \\mathbb{R}^{d_{{m}} \\times d_{{m}}}$ together with biases $\\mathbf{b}_{\\mathcal{S}}^l, \\mathbf{b}_{\\mathcal{T}}^l, \\mathbf{b}_{\\mathcal{N}}^l, \\mathbf{b}_{\\mathcal{O}}^l \\in \\mathbb{R}^{d_{m}}$ to obtain new speaker-specific hidden representations $\\hat{\\mathbf{Z}}^l = [\\hat{\\mathbf{z}}^l_1, \\ldots, \\hat{\\mathbf{z}}^l_T]$ as:\n",
    "\n",
    "$\\hat{\\mathbf{z}}^l_t = \\left( \\mathbf{W}_{\\mathcal{S}}^l \\mathbf{z}^l_t + \\mathbf{b}_{\\mathcal{S}}^l \\right) p^t_{\\mathcal{S}} +\n",
    "\\left( \\mathbf{W}_{\\mathcal{T}}^l \\mathbf{z}^l_t + \\mathbf{b}_{\\mathcal{T}}^l \\right) p^t_{\\mathcal{T}}  \\nonumber \\\\\n",
    " + \\left( \\mathbf{W}_{\\mathcal{N}}^l \\mathbf{z}^l_t + \\mathbf{b}_{\\mathcal{N}}^l\\right) p^t_{\\mathcal{N}} +\n",
    "\\left( \\mathbf{W}_{\\mathcal{O}}^l \\mathbf{z}^l_t + \\mathbf{b}_{\\mathcal{O}}^l \\right) p^t_{\\mathcal{O}}.$\n",
    "\n",
    "\n",
    "In other words, the hidden representations $\\mathbf{z}^l_t$ are transformed using a convex combination of the four STNO class-specific affine transformations, weighted by the corresponding STNO class probabilities.\n",
    "\n",
    "![dicow_full](img/target_speaker_whisper_stno.drawio.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64edb329c931eea9",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Environment Setup\n",
    "Letâ€™s get things rolling by setting up your environment for training and evaluating the DiCoW model on the Libri2Mix dataset.\n",
    "Weâ€™ll install all necessary dependencies and clone the repository.\n",
    "\n",
    "### Step 1: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "id": "935d8dac269925a9",
   "metadata": {},
   "source": [
    "with open(\"reqs_collab.txt\", \"w\") as f:\n",
    "    f.write(\n",
    "        \"\"\"accelerate>=0.33.0\n",
    "          datasets>=2.21.0\n",
    "          evaluate>=0.4.2\n",
    "          huggingface-hub==0.24.6\n",
    "          hydra-core==1.3.2\n",
    "          intervaltree==3.1.0\n",
    "          jiwer==2.5.2\n",
    "          kaldiio==2.18.0\n",
    "          lhotse==1.28.0\n",
    "          librosa==0.10.2.post1\n",
    "          meeteval==0.3.0\n",
    "          pandas==2.2.2\n",
    "          pyannote.core==5.0.0\n",
    "          pyannote.database==5.1.0\n",
    "          pyannote.metrics==3.2.1\n",
    "          PyYAML==6.0.2\n",
    "          transformers==4.41.2\n",
    "          wandb>=0.19.0\n",
    "          simplejson==3.20.1\n",
    "          \"\"\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7e2c2e40b67354b3",
   "metadata": {},
   "source": [
    "# Install required packages\n",
    "!pip install -q -r reqs_collab.txt\n",
    "# !pip install -q gdown\n",
    "!pip uninstall peft -y\n",
    "!pip uninstall tensorflow -y"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "(Optionally) Use TensorBoard for monitoring training progress.",
   "id": "c80ebdbc899102cd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# !pip install tensorboard --force-reinstall",
   "id": "909c2e87b5f5f9d9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# %load_ext tensorboard\n",
    "# %tensorboard --logdir=/content/TS-ASR-Whisper/exp/lsmix_tiny/runs"
   ],
   "id": "b3afb78435e7b909"
  },
  {
   "cell_type": "markdown",
   "id": "cc10c199c364f9c5",
   "metadata": {},
   "source": [
    "### Step 2: Clone the Repository\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "a03cf868e7990630",
   "metadata": {},
   "source": [
    "# Clone the DiCoW repository\n",
    "!git clone https://github.com/BUTSpeechFIT/TS-ASR-Whisper.git\n",
    "%cd TS-ASR-Whisper\n",
    "!git checkout JSALT25_demo\n",
    "\n",
    "# Initialize and update submodules\n",
    "!git submodule init\n",
    "!git submodule update\n",
    "%cd .."
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "54d6af60297d2dd1",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Data Preparation\n",
    "\n",
    "Time to prep the data!\n",
    "Weâ€™ll download, unzip, and organize the Libri2Mix dataset so itâ€™s ready for training and evaluation.\n",
    "\n",
    "#### 1. Prepare directories\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "22ffef360722427b",
   "metadata": {},
   "source": [
    "!mkdir -p data\n",
    "!mkdir -p data/libri2mix\n",
    "!mkdir -p data/manifests\n",
    "!mkdir -p data/libri2mix/train-100\n",
    "!mkdir -p data/libri2mix/dev\n",
    "!mkdir -p data/libri2mix/test"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "6d5c10cda51cbe49",
   "metadata": {},
   "source": [
    "#### 2. Download prepared Libri2Mix 100h clean dataset\n",
    "\n",
    "There are **five download links** available for the Libri2Mix 100-hour clean dataset. Please **select the link based on where you're sitting in the classroom**:\n",
    "\n",
    "* **First path:** If you're seated on the **far left side** of the room.\n",
    "* **Second path:** If you're in the **middle of the left wing**.\n",
    "* **Third path:** If you're in the **center of the room**.\n",
    "* **Fourth path:** If you're in the **middle of the right wing**.\n",
    "* **Fifth path:** If you're seated on the **far right side** of the room.\n",
    "\n",
    "Use the corresponding [Google Drive link](https://drive.google.com/drive/folders/1ESYPaPBALrckPtqmV7b6YZPWfGrbb-9n?usp=sharing) for your area.\n",
    "\n",
    "âš ï¸ *If you encounter a bandwidth quota issue on Google Drive*, donâ€™t worry â€” simply use the provided bash script to download the dataset directly from our **Nextcloud** server instead.\n",
    "\n",
    "The dataset is ready-to-use and includes 100 hours of clean, mixed speech derived from LibriSpeech.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "7bb11b0dbd338e2c",
   "metadata": {},
   "source": [
    "import gdown\n",
    "\n",
    "gdown.download_folder(\"https://drive.google.com/drive/folders/1vZEroIOIa2H8JqAltGxFebBv_ukiKU4j?usp=sharing\", use_cookies=False, quiet=True, output=\"data/libri2mix\")\n",
    "# gdown.download_folder(\"https://drive.google.com/drive/folders/1cI4r6Eq_uJscAQ55hUOMURadD50EiBKz?usp=sharing\", use_cookies=False, quiet=True, output=\"data/libri2mix\")\n",
    "# gdown.download_folder(\"https://drive.google.com/drive/folders/1cOqYPowsXC76IYlkPPAH9E1u_KOHrQ3r?usp=sharing\", use_cookies=False, quiet=True, output=\"data/libri2mix\")\n",
    "# gdown.download_folder(\"https://drive.google.com/drive/folders/1zNaH3eE5Gp9h65dBFlzpNJZgcnWA36Hw?usp=sharing\", use_cookies=False, quiet=True, output=\"data/libri2mix\")\n",
    "# gdown.download_folder(\"https://drive.google.com/drive/folders/1ESYPaPBALrckPtqmV7b6YZPWfGrbb-9n?usp=sharing\", use_cookies=False, quiet=True, output=\"data/libri2mix\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-16T15:41:03.276280Z",
     "start_time": "2025-06-16T15:41:03.201512Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%%bash\n",
    "# Uncomment the following lines to download the dataset from Nextcloud server\n",
    "# Cutsets\n",
    "# curl -L -o data/libri2mix/libri2mix_mix_clean_sc_dev_cutset.jsonl.gz https://nextcloud.fit.vutbr.cz/s/MHLxjrd8XWPCieE/download\n",
    "# curl -L -o data/libri2mix/libri2mix_clean_100_train_sc_cutset_30s.jsonl.gz https://nextcloud.fit.vutbr.cz/s/gyPBwcMM3Azqbpk/download\n",
    "# curl -L -o data/libri2mix/libri2mix_mix_clean_sc_test_cutset.jsonl.gz https://nextcloud.fit.vutbr.cz/s/gdDMe2EKdAn4Kx5/download\n",
    "\n",
    "# Data\n",
    "# curl -L -o data/libri2mix/train_mix_clean.tar.gz https://nextcloud.fit.vutbr.cz/s/oXkxkW59xDKLPgJ/download\n",
    "# curl -L -o data/libri2mix/dev_mix_clean.tar.gz https://nextcloud.fit.vutbr.cz/s/DmiAicG2aWLeLqm/download\n",
    "# curl -L -o data/libri2mix/test_mix_clean.tar.gz https://nextcloud.fit.vutbr.cz/s/WACLidXg78BgesB/download"
   ],
   "id": "d52dfb5570180316",
   "outputs": [],
   "execution_count": 17
  },
  {
   "cell_type": "markdown",
   "id": "609c487c91cedc7",
   "metadata": {},
   "source": [
    "#### 3. Unzip downloaded datasets"
   ]
  },
  {
   "cell_type": "code",
   "id": "22ec580fa230ffc",
   "metadata": {},
   "source": [
    "!tar -xzf data/libri2mix/train_mix_clean.tar.gz -C data/libri2mix/train-100\n",
    "!tar -xzf data/libri2mix/dev_mix_clean.tar.gz -C data/libri2mix/dev\n",
    "!tar -xzf data/libri2mix/test_mix_clean.tar.gz -C data/libri2mix/test"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "333a5713ebb66f30",
   "metadata": {},
   "source": [
    "#### 4. Fix paths in the dataset manifests"
   ]
  },
  {
   "cell_type": "code",
   "id": "d710c87717769ff3",
   "metadata": {},
   "source": [
    "from lhotse import load_manifest\n",
    "import os\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    for cutset, out in [\n",
    "        (\"data/libri2mix/libri2mix_clean_100_train_sc_cutset_30s.jsonl.gz\",\n",
    "         \"data/manifests/libri2mix_clean_100_train_sc_cutset_30s.jsonl.gz\"),\n",
    "        (\"data/libri2mix/libri2mix_mix_clean_sc_dev_cutset.jsonl.gz\",\n",
    "         \"data/manifests/libri2mix_mix_clean_sc_dev_cutset.jsonl.gz\"),\n",
    "        (\"data/libri2mix/libri2mix_mix_clean_sc_test_cutset.jsonl.gz\",\n",
    "         \"data/manifests/libri2mix_mix_clean_sc_test_cutset.jsonl.gz\")]:\n",
    "        cset = load_manifest(cutset)\n",
    "        for r in cset:\n",
    "            for src in r.recording.sources:\n",
    "                src.source = src.source.replace(\n",
    "                    'PATH_TO_BE_REPLACED',\n",
    "                    os.path.abspath('data/libri2mix'))\n",
    "            for s in r.supervisions:\n",
    "                s.alignment = None\n",
    "\n",
    "        cset.to_file(out)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "2c4969a03ae4b5a",
   "metadata": {},
   "source": [
    "#### 5. Check the dataset\n",
    "Hereâ€™s what youâ€™ll find in the training cutset:\n",
    "\n",
    "* ğŸ’¬ 100 hours of clean, preprocessed speech\n",
    "* ğŸ™ï¸ Each cut contains audio with overlapping speakers\n",
    "* ğŸ“ Supervisions include speaker IDs and transcriptions\n",
    "\n",
    "This subset was created by mixing two LibriSpeech samples into one audio file.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "f5800c645f5506aa",
   "metadata": {},
   "source": [
    "from lhotse import load_manifest\n",
    "\n",
    "train_cutset = load_manifest(\"data/manifests/libri2mix_clean_100_train_sc_cutset_30s.jsonl.gz\")\n",
    "train_cutset.describe()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Want to see (and hear) what the data looks like?\n",
    "Each cut includes a mixed audio recording with annotations for every speaker involved."
   ],
   "id": "373ae38ea4460a2b"
  },
  {
   "cell_type": "code",
   "id": "6bffa6c4d91416dc",
   "metadata": {},
   "source": [
    "sample = train_cutset[0]\n",
    "print(f\"{sample.supervisions[0].speaker}: {sample.supervisions[0].text}\")\n",
    "print(f\"{sample.supervisions[1].speaker}: {sample.supervisions[1].text}\")\n",
    "sample.plot_audio()\n",
    "sample.play_audio()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### 6. Try It in the DiCoW Gradio App:\n",
    "Want a quick test without training anything?\n",
    "Submit a sample file to the DiCoW [Gradio demo app](https://pccnect.fit.vutbr.cz/gradio-demo/) and see how the pretrained model transcribes overlapping speakers.\n",
    "ğŸ§ You can also upload your own audio to test it live!"
   ],
   "id": "da364a439eb2b02c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torchaudio\n",
    "import torch\n",
    "waveform = sample.load_audio()\n",
    "torchaudio.save(\"l2mix_sample.wav\", src=torch.tensor(waveform), sample_rate=16_000)"
   ],
   "id": "d02f49a74177edfa",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "78084b5673b247ac",
   "metadata": {},
   "source": [
    "#### 7. Prepare a Small Dev Set for Quick Testing\n",
    "For quick iteration, you can create a small development set.\n",
    "A sample of **128 cuts** is usually enough to get a rough performance estimate.\n",
    "(If you prefer full accuracy, use the complete dev set with 3000 cuts.).\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "19c4050ada452286",
   "metadata": {},
   "source": [
    "from lhotse import load_manifest\n",
    "\n",
    "devset = load_manifest(\"data/manifests/libri2mix_mix_clean_sc_dev_cutset.jsonl.gz\")\n",
    "devset =devset.subset(first=128)\n",
    "devset.to_file(\"data/manifests/libri2mix_mix_clean_sc_dev_cutset_100.jsonl.gz\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "334546767fbc82d3",
   "metadata": {},
   "source": [
    "import os\n",
    "MANIFEST_DIR = os.path.abspath(\"data/manifests\")\n",
    "os.environ[\"MANIFEST_DIR\"] = MANIFEST_DIR\n",
    "os.environ[\"TRAIN_CUTSET\"] = f\"{MANIFEST_DIR}/libri2mix_clean_100_train_sc_cutset_30s.jsonl.gz\"\n",
    "os.environ[\"DEV_CUTSET\"] = f\"{MANIFEST_DIR}/libri2mix_mix_clean_sc_dev_cutset.jsonl.gz\"\n",
    "os.environ[\"TEST_CUTSET\"] = f\"{MANIFEST_DIR}/libri2mix_mix_clean_sc_test_cutset.jsonl.gz\"\n",
    "os.environ[\"TOYSET_CUTSET\"] = f\"{MANIFEST_DIR}/libri2mix_mix_clean_sc_dev_cutset_100.jsonl.gz\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "bc56dfe2c60bc368",
   "metadata": {},
   "source": [
    "----\n",
    "## 4. Model Training\n",
    "Letâ€™s make sure everything is working before we commit to a full training run."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "%cd TS-ASR-Whisper",
   "id": "b43169b84590ab6e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "#### Step 1: Prepare the decoding configuration file"
   ],
   "id": "fdf0a796d11dd74f"
  },
  {
   "cell_type": "code",
   "id": "bd7a7500d851f35",
   "metadata": {},
   "source": [
    "with open(\"configs/decode/toyset_decoding.yaml\", \"w\") as f:\n",
    "    f.write(\n",
    "\"\"\"\n",
    "# @package _global_\n",
    "experiment: libri2mix_decode_both\n",
    "\n",
    "model:\n",
    "  whisper_model: \"openai/whisper-tiny\"\n",
    "data:\n",
    "  eval_cutsets: \"${oc.env:TOYSET_CUTSET}\"\n",
    "  train_cutsets: \"${oc.env:TOYSET_CUTSET}\"\n",
    "  dev_cutsets: \"${oc.env:TOYSET_CUTSET}\"\n",
    "  eval_text_norm: \"whisper_nsf\"\n",
    "training:\n",
    "  decode_only: true\n",
    "  bf16: false\n",
    "  bf16_full_eval: false\n",
    "  eval_metrics_list: [ \"tcp_wer\", \"cp_wer\"]\n",
    "  per_device_eval_batch_size: 16\n",
    "  dataloader_num_workers: 4\n",
    "  dataloader_prefetch_factor: 1\n",
    "  dataloader_pin_memory: true\n",
    "\"\"\"\n",
    "    )"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "3b90b916da08a6cb",
   "metadata": {},
   "source": [
    "##### Step 2: Export the environment variables for decoding"
   ]
  },
  {
   "cell_type": "code",
   "id": "1b72685706ed864d",
   "metadata": {},
   "source": [
    "import os\n",
    "os.environ[\"SRC_ROOT\"] = os.path.abspath(\".\")\n",
    "os.environ[\"WANDB_ANONYMOUS\"] = \"allow\"\n",
    "# os.environ[\"WANDB_ENTITY\"] = \"\"  # Set your Weights & Biases entity\n",
    "os.environ[\"WANDB_PROJECT\"] = \"DiCoW_playground\"\n",
    "os.environ[\"WANDB_RUN_ID\"] = \"libri2mix_decode_both\"\n",
    "os.environ[\"HF_HOME\"] = \"hf_cache\"\n",
    "os.environ[\"PYTHONPATH\"] = f\"{os.environ['SRC_ROOT']}\"\n",
    "os.environ[\"EXPERIMENT_PATH\"] = f\"{os.environ['SRC_ROOT']}/exp/{os.environ.get('EXPERIMENT', '')}\"\n",
    "os.environ[\"LIBRI_TRAIN_CACHED_PATH\"] = \"\"\n",
    "os.environ[\"LIBRI_DEV_CACHED_PATH\"] = \"\"\n",
    "os.environ[\"AUDIO_PATH_PREFIX\"] = \"\"\n",
    "os.environ[\"AUDIO_PATH_PREFIX_REPLACEMENT\"] = \"\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Step 3: Run the decoding script\n",
    "Start by checking that your environment is correctly set up.\n",
    "Try running the decoding script on a small dataset.\n",
    "Youâ€™ll notice that a randomly initialized model performs poorly â€” no surprise there! Letâ€™s fix that with some fine-tuning.\n"
   ],
   "id": "c390cc51fcc27b05"
  },
  {
   "cell_type": "code",
   "id": "a1e975bfec102486",
   "metadata": {},
   "source": "!python src/main.py +decode=toyset_decoding",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Step 4: Check the decoding results via meeteval",
   "id": "a1bf25bcf4c80f7f"
  },
  {
   "cell_type": "code",
   "id": "8be916aea2135c33",
   "metadata": {},
   "source": [
    "import meeteval\n",
    "from meeteval.viz.visualize import AlignmentVisualization\n",
    "\n",
    "folder = r'exp/libri2mix_decode_both/test/0/wer/1919-142785-0014_3000-15664-0027'\n",
    "av = AlignmentVisualization(\n",
    "    meeteval.io.load(folder + '/ref.json'),\n",
    "    meeteval.io.load(folder + '/tcp_wer_hyp.json')\n",
    ")\n",
    "display(av)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### 5. DiCoW Training\n",
    "Now itâ€™s time to train!\n",
    "Weâ€™ll use the **tiny Whisper model** as the base and fine-tune it with DiCoW on the Libri2Mix dataset.\n",
    "\n",
    "Create a training config for the `DiCoW-tiny` model and launch your training run.\n",
    "\n",
    "ğŸ You can stop training anytime you're happy with the results and jump to decoding.\n"
   ],
   "id": "a139b4545bd6741d"
  },
  {
   "cell_type": "code",
   "id": "ccf4b76a28d37366",
   "metadata": {},
   "source": [
    "with open(\"configs/train/tiny_l2mix.yaml\", \"w\") as f:\n",
    "    f.write(\n",
    "\"\"\"\n",
    "# @package _global_\n",
    "defaults:\n",
    "  - /train/icassp/table1_model_comparisons/base\n",
    "\n",
    "experiment: lsmix_tiny\n",
    "wandb:\n",
    "  project: jsalt25_dicow_challenge\n",
    "model:\n",
    "  whisper_model: openai/whisper-tiny\n",
    "  reinit_encoder_from: null\n",
    "  target_amp_bias_only: true\n",
    "  target_amp_use_silence: false\n",
    "  target_amp_use_target: true\n",
    "  target_amp_use_overlap: false\n",
    "  target_amp_use_non_target: false\n",
    "data:\n",
    "  train_text_norm: \"whisper_nsf\"\n",
    "  use_timestamps: true\n",
    "  eval_cutsets: \"${oc.env:TEST_CUTSET}\"\n",
    "  train_cutsets: \"${oc.env:TRAIN_CUTSET}\"\n",
    "  dev_cutsets: \"${oc.env:TOYSET_CUTSET}\"\n",
    "  eval_text_norm: \"whisper_nsf\"\n",
    "\n",
    "training:\n",
    "  warmup_steps: 2000\n",
    "  remove_timestamps_from_ctc: true\n",
    "  overall_batch_size: 24\n",
    "  learning_rate: 1e-5\n",
    "  per_device_eval_batch_size: 16\n",
    "  bf16: false\n",
    "  bf16_full_eval: false\n",
    "  fp16: true\n",
    "  eval_metrics_list: [ \"tcp_wer\", \"cp_wer\"]\n",
    "  eval_strategy: steps\n",
    "  save_strategy: steps\n",
    "  eval_steps: 200\n",
    "  save_steps: 200\n",
    "  use_amplifiers_only_n_epochs: 0\n",
    "  report_to: [\"tensorboard\"]\n",
    "\"\"\"\n",
    "    )"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Are you paying attention? ğŸ˜ˆ\n",
    "\n",
    "If not, you might miss an important detail: **we are *not* using FDDTs properly in this config**.\n",
    "Take a look at the `TargetSpeakerAmplifier` class in [`src/models/whisper_ctc.py`](src/models/whisper_ctc.py).\n",
    "\n",
    "> ğŸ’€ Oh no â€” it's even worse: **FDDTs aren't fully implemented** (yet).\n",
    "> Follow the equation above, or dive into the codebase here:\n",
    "> ğŸ‘‰ [https://github.com/BUTSpeechFIT/TS-ASR-Whisper](https://github.com/BUTSpeechFIT/TS-ASR-Whisper)\n",
    "\n",
    "While you're there, feel free to â­ star the repo if you find it useful.\n"
   ],
   "id": "de700a03b35d13c7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Ready to start training?\n",
    "\n",
    "Oh nooooooo! ğŸ˜± *One more thing...*\n",
    "Looks like someone went off the rails and decided to **break your code by adding todayâ€™s date to the STNO masks**. ğŸ˜­\n",
    "\n",
    "Letâ€™s fix that:\n",
    "Check the `src/data/local_datasets.py` script â€” that's where the STNO masks are computed.\n"
   ],
   "id": "e60b0a5ed0454a0c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "!python src/main.py +train=tiny_l2mix",
   "id": "23dd94a18f917d0c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Not happy with the results? Perfect. That means you're ready to experiment. ğŸ˜ˆ\n",
    "\n",
    "#### Quick tweaks to try:\n",
    "\n",
    "* ğŸ” Swap in the pretrained CTC checkpoint - https://nextcloud.fit.vutbr.cz/s/2gzG7RyH3Pog734/download.\n",
    "* âš™ï¸ Tweak training params: batch size, learning rate, warmup steps (`configs/base.yaml` has you covered).\n",
    "* ğŸ§ª Pretrain FDDT parameters for 1â€“2 epochs via `use_amplifiers_only_n_epochs`.\n",
    "* ğŸ¯ Use *bias-only* mode for FDDT (`target_amp_is_diagonal`, `target_amp_bias_only`).\n",
    "* ğŸ“‰ Reduce the number of FDDT layers with `apply_target_amp_to_n_layers`.\n",
    "* âš–ï¸ Adjust the CTC loss weight via `ctc_weight`.\n",
    "* ğŸš€ Enable joint CTC/Attention beam decoding with `generation_num_beams`, `decoding_ctc_weight`, and play with length penalty & friends.\n",
    "Hereâ€™s a tightened-up, well-formatted version that keeps the humor and clarity intact:\n",
    "\n",
    "\n",
    "#### Want more of a challenge?\n",
    "\n",
    "* ğŸ§¨ **Inject noise into the STNO masks**\n",
    "  File: `TS-ASR-Whisper/src/data/local_datasets.py`\n",
    "  Youâ€™ve already mastered STNO masks â€” so this should be easy... right? ğŸ˜\n",
    "\n",
    "* ğŸ§¬ **Change where FDDT is applied inside the model**\n",
    "  File: `TS-ASR-Whisper/src/models/whisper_ctc.py`\n",
    "  Try suppressing the signal *before* adding positional embeddings. Or *after* the first layer. Or maybe *right before* the final layer. Be bold.\n",
    "\n",
    "* ğŸ§  **Make TS-ASR instances aware of each other**\n",
    "  File: `TS-ASR-Whisper/src/models/whisper_ctc.py`\n",
    "  Not bored yet? Take a look at Section 4.5: *Co-Attention Module for Speaker Interaction* in the DiCoW paper.\n",
    "  Itâ€™s like group therapy for your speakers. ğŸ¤\n",
    "\n",
    "* ğŸ¤– **Use LLMs to postprocess model outputs** -\n",
    "  Because why not let GPT clean up your mess?\n",
    "  *Actually... nah, donâ€™t do that.*\n",
    "\n",
    "\n"
   ],
   "id": "69515829a324079e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---\n",
    "## 5. Decoding & Evaluation\n",
    "\n",
    "Congrats â€” youâ€™ve officially trained the DiCoW model on Libri2Mix! ğŸ‰ğŸ‘\n",
    "Time to see if it actually works. ğŸ˜…\n",
    "\n",
    "Letâ€™s decode the test set and find out how well (or weirdly) your model performs. ğŸ”ğŸ“Š"
   ],
   "id": "ced6b00bf8e45e1e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "with open(\"configs/decode/tiny_test.yaml\", \"w\") as f:\n",
    "    f.write(\n",
    "\"\"\"\n",
    "# @package _global_\n",
    "experiment: libri2mix_clean\n",
    "\n",
    "model:\n",
    "  whisper_model: \"openai/whisper-tiny\"\n",
    "  reinit_from: \"/content/TS-ASR-Whisper/exp/lsmix_tiny/checkpoint-600/model.safetensors\"\n",
    "data:\n",
    "  eval_cutsets: \"${oc.env:TEST_CUTSET}\"\n",
    "  train_cutsets: \"${oc.env:TOYSET_CUTSET}\"\n",
    "  dev_cutsets: \"${oc.env:TOYSET_CUTSET}\"\n",
    "  eval_text_norm: \"whisper_nsf\"\n",
    "training:\n",
    "  decode_only: true\n",
    "  bf16: false\n",
    "  bf16_full_eval: false\n",
    "  dataloader_num_workers: 4\n",
    "  dataloader_prefetch_factor: 1\n",
    "  dataloader_pin_memory: true\n",
    "  generation_max_length: 64\n",
    "  eval_metrics_list: [ \"tcp_wer\", \"cp_wer\"]\n",
    "  per_device_eval_batch_size: 64\n",
    "\"\"\"\n",
    "    )"
   ],
   "id": "e1bf6daac9b8343",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "!python src/main.py +decode=tiny_test",
   "id": "2ffc4b80fb32a8d9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---\n",
    "## 6. Generate Submission\n",
    "\n",
    "Youâ€™ve decoded, evaluated, and now itâ€™s time for the grand finale: submission time! ğŸ“¤\n",
    "\n",
    "Gather all your hypotheses and create that shiny submission file. Once itâ€™s ready, head over to the [EMMA Leaderboard](https://huggingface.co/spaces/BUT-FIT/EMMA_leaderboard) and show the world what your model can do."
   ],
   "id": "f8bbf450e53ff61d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import meeteval\n",
    "import glob\n",
    "hyps = []\n",
    "for hyp_file in glob.glob(\"/content/TS-ASR-Whisper/exp/libri2mix_clean/test/0/wer/*/tcp_wer_hyp.json\"):\n",
    "    hyps.append(meeteval.io.load(hyp_file))\n",
    "hyp = meeteval.io.SegLST.merge(*hyps)\n",
    "hyp.dump(f\"/content/my_submission.json\")"
   ],
   "id": "3ef4f46bb4395423",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Congrats â€” youâ€™ve officially trained and evaluated the DiCoW model on Libri2Mix. ğŸ¥³\n",
    "\n",
    "Now go claim your spot on the leaderboardâ€¦ and maybe a prize (or at least a celebratory beer). ğŸºğŸºğŸº\n",
    "\n",
    "Personally? Iâ€™m already thirsty."
   ],
   "id": "76f0ee12fb479f4a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "839b61c77dfb24d7"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
