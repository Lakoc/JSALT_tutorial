{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a77807f92f26ee",
   "metadata": {},
   "source": [
    "# DiCoW: Diarization-Conditioned Whisper for Target Speaker ASR - JSALT 25 Competition\n",
    "\n",
    "## üèÜ Competition Information\n",
    "Welcome to the DiCoW Target Speaker ASR Challenge!\n",
    "\n",
    "**Prizes:**\n",
    "- ü•á **1st Place:** 3 beers üç∫üç∫üç∫\n",
    "- ü•à **2nd Place:** 2 beers üç∫üç∫\n",
    "- ü•â **3rd Place:** 1 beer üç∫\n",
    "\n",
    "---\n",
    "**Submission:** Submit your best performing systems to the [EMMA Leaderboard](https://huggingface.co/spaces/BUT-FIT/EMMA_leaderboard)\n",
    "\n",
    "**Deadline:** 17.6. 2025, 16:00 CET\n",
    "\n",
    "**SUBMISSION_TOKEN:** emmA2025\n",
    "\n",
    "---\n",
    "\n",
    "## üõ†Ô∏è Challenge Tasks\n",
    "1. Clone the DiCoW repository and set up the environment\n",
    "2. Prepare the Libri2Mix dataset\n",
    "3. Finetune Whisper tiny model using DiCoW\n",
    "4. Evaluate on Libri2Mix clean test set\n",
    "5. Submit results to EMMA leaderboard\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Table of Contents\n",
    "1. [Introduction to MT-ASR, TS-ASR, and DiCoW](#intro)\n",
    "    1. [Multi-Talker ASR (MT-ASR)](#mt_asr)\n",
    "    2. [Target Speaker ASR (TS-ASR)](#ts_asr)\n",
    "    3. [DiCoW: Diarization-Conditioned Whisper](#dicow)\n",
    "2. [Environment Setup](#setup)\n",
    "3. [Data Preparation](#data)\n",
    "4. [Model Training](#finetuning)\n",
    "5. [Decoding & Evaluation](#decoding)\n",
    "6. [Submission Guidelines](#submission)\n",
    "\n",
    "---\n",
    "\n",
    "## üìñ Resources\n",
    "1. Repositories:\n",
    "    - [DiCoW GitHub Repository](https://github.com/BUTSpeechFIT/TS-ASR-Whisper)\n",
    "    - [DiCoW Inference Repository](https://github.com/BUTSpeechFIT/DiCoW)\n",
    "2. Papers:\n",
    "    - [Target Speaker ASR with Whisper](https://ieeexplore.ieee.org/document/10887683)\n",
    "    - [DiCoW: Diarization-Conditioned Whisper for Target Speaker ASR](https://arxiv.org/abs/2501.00114)\n",
    "    - [BUT/JHU System Description for CHiME-8 NOTSOFAR-1 Challenge](https://www.isca-archive.org/chime_2024/polok24_chime.html)\n",
    "    - BUT System for the MLC-SLM Challenge\n",
    "3. Demo:\n",
    "    - [DiCoW Gradio Demo](https://pccnect.fit.vutbr.cz/gradio-demo/)\n",
    "4. Leaderboard:\n",
    "    - [EMMA Leaderboard](https://huggingface.co/spaces/BUT-FIT/EMMA_leaderboard)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9403faf6ea69280d",
   "metadata": {},
   "source": [
    "# 1. INTRODUCTION TO MULTI-TALKER AUTOMATIC SPEECH RECOGNITION <a id='#intro'></a>\n",
    "\n",
    "\n",
    "![mt_asr](img/mt_asr.png)\n",
    "\n",
    "## The Challenge of Multi-Talker ASR\n",
    "\n",
    "Automatic Speech Recognition (ASR) systems traditionally work well with single-speaker audio.\n",
    "However, real-world scenarios often involve multiple speakers talking simultaneously, creating\n",
    "several challenges:\n",
    "\n",
    "1. **Overlapping Speech**: Multiple speakers talking at the same time\n",
    "2. **Speaker Confusion**: Difficulty determining who said what\n",
    "3. **Acoustic Interference**: Speech from one speaker masks another\n",
    "4. **Variable Number of Speakers**: Unknown number of active speakers\n",
    "\n",
    "\n",
    "### Approaches to Multi-Talker ASR <a id='#mt_asr'></a>\n",
    "\n",
    "![mt_asr_approaches](img/mt_asr_approaches.png)\n",
    "\n",
    "\n",
    "1. **Speech Separation + ASR**: First separate speakers, then apply ASR\n",
    "2. **E2E MT-ASR (SOT)**: Concatenate speaker-attributed transcriptions by emission time\n",
    "3. **Target Speaker ASR**: Focus on specific speaker of interest\n",
    "\n",
    "### Metrics for Multi-Talker ASR\n",
    "1. Optimal Reference Combination Word Error Rate (ORC WER)\n",
    "2. Concatenated minimum-Permutation Word Error Rate (cpWER)\n",
    "3. Time-Constrained minimum-Permutation Word Error Rate (tcpWER)\n",
    "\n",
    "https://github.com/fgnt/meeteval\n",
    "\n",
    "https://groups.uni-paderborn.de/nt/meeteval/icassp2024-demo/poster_example.html?selection=28.6-38.7&minimaps=1\n",
    "\n",
    "## Target Speaker ASR: Focus on Speaker of Interest\n",
    "<a id='ts_asr'></a>\n",
    "Target Speaker ASR (TS-ASR) addresses a practical scenario: given mixed audio with multiple\n",
    "speakers, transcribe only the speech from a specific target speaker.\n",
    "\n",
    "\n",
    "### Traditional TS-ASR Approaches:\n",
    "\n",
    "1. Randomly initialized model and i-vector based speaker embeddings\n",
    "\n",
    "![ts_asr_embed](img/ts_asr_embed.png)\n",
    "\n",
    "2. Pretrained ASR model with (better) speaker embeddings\n",
    "\n",
    "![ts_asr_enrolment](img/ts_asr_enrolment.png)\n",
    "\n",
    "3. Pretrained ASR model directly conditioned on speaker enrolment\n",
    "\n",
    "![whisper_enrolment](img/whisper_enrolment.png)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12bfa19f046f163c",
   "metadata": {},
   "source": [
    "<a id='dicow'></a>\n",
    "## DICOW: DIARIZATION-CONDITIONED WHISPER\n",
    "\n",
    "DiCoW (Diarization-Conditioned Whisper) represents a paradigm shift in Target Speaker ASR.\n",
    "Instead of relying on speaker embeddings, DiCoW leverages speaker diarization outputs as\n",
    "conditioning information.\n",
    "\n",
    "![dicow](img/dicow.png)\n",
    "\n",
    "### Advantages of DiCoW\n",
    "\n",
    "1. **No Speaker Embeddings Required**: Eliminates dependency on embedding quality\n",
    "2. **Better Generalization**: Works well with unseen speakers\n",
    "3. **Simplified Workflow**: Direct conditioning on diarization outputs\n",
    "4. **Maintains Whisper Performance**: Preserves accuracy on (multi-lingual) single-speaker data\n",
    "\n",
    "### STNO - Silence, Target, Non-Target, and Overlap Masks\n",
    "\n",
    "Let $\\mathbf{D} \\in [0,1]^{S \\times T}$, where $S$ is the number of speakers in the recording, and $T$ is the number of frames, represent the diarization output, with $d(s, t)$ denoting the probability that speaker $s$ is active in time frame $t$. Let $s_k$ represent the target speaker.\n",
    "We define a distribution over the following mutually exclusive events for a frame at time $t$:\n",
    "\n",
    "1. ${\\mathcal{S}}$: Time frame $t$ represents silence.\n",
    "2. ${\\mathcal{T}}$: The target speaker, $s_k$, is the only active speaker in time frame $t$.\n",
    "3. ${\\mathcal{N}}$: One or more non-target speakers, $s \\neq s_k$ are active and the target speaker, $s_k$, is not active at time frame $t$.\n",
    "4. ${\\mathcal{O}}$: The target speaker $s_k$ is active while at least one non-target speaker $s \\neq s_k$ is also active at time frame $t$, denoting an overlap.\n",
    "\n",
    "\n",
    "The probabilities of these events occurring at time frame $t$ can be calculated as:\n",
    "1. $p_{\\mathcal{S}}^t  = \\prod_{s=1}^S (1 - d(s, t))$\n",
    "2. $p_{\\mathcal{T}}^t  = d(s_k, t)  \\prod_{\\substack{s=1 \\\\ s \\neq s_k}}^S (1 - d(s, t))$\n",
    "3. $p_{\\mathcal{N}}^t  = \\left(1 - p_{\\mathcal{S}}^t\\right) - d\\left(s_k, t\\right)$\n",
    "4. $p_{\\mathcal{O}}^t  = d(s_k, t) - p_{\\mathcal{T}}^t$\n",
    "\n",
    "\n",
    "This definition allows us to use a fixed-sized STNO (Silence, Target, Non-target, Overlap) mask $\\mathbf{M}^t = \\begin{bmatrix} p_{\\mathcal{S}}^t & p_{\\mathcal{T}}^t & p_{\\mathcal{N}}^t & p_{\\mathcal{O}}^t \\end{bmatrix}^{\\top}$.\n",
    "\n",
    "### Frame-Level Diarization Dependent Transformations\n",
    "\n",
    "\n",
    "Let $\\mathbf{Z}^l \\in \\mathbb{R}^{d_{{m}} \\times T}$ represent the frame-by-frame inputs to the $l$-th (Transformer) layer.\n",
    "\n",
    "We transform these hidden representations by applying four affine STNO layer- and class-specific transformations: $\\mathbf{W}_{\\mathcal{S}}^l, \\mathbf{W}_{\\mathcal{T}}^l, \\mathbf{W}_{\\mathcal{N}}^l, \\mathbf{W}_{\\mathcal{O}}^l \\in \\mathbb{R}^{d_{{m}} \\times d_{{m}}}$ together with biases $\\mathbf{b}_{\\mathcal{S}}^l, \\mathbf{b}_{\\mathcal{T}}^l, \\mathbf{b}_{\\mathcal{N}}^l, \\mathbf{b}_{\\mathcal{O}}^l \\in \\mathbb{R}^{d_{m}}$ to obtain new speaker-specific hidden representations $\\hat{\\mathbf{Z}}^l = [\\hat{\\mathbf{z}}^l_1, \\ldots, \\hat{\\mathbf{z}}^l_T]$ as:\n",
    "\n",
    "$\\hat{\\mathbf{z}}^l_t = \\left( \\mathbf{W}_{\\mathcal{S}}^l \\mathbf{z}^l_t + \\mathbf{b}_{\\mathcal{S}}^l \\right) p^t_{\\mathcal{S}} +\n",
    "\\left( \\mathbf{W}_{\\mathcal{T}}^l \\mathbf{z}^l_t + \\mathbf{b}_{\\mathcal{T}}^l \\right) p^t_{\\mathcal{T}}  \\nonumber \\\\\n",
    " + \\left( \\mathbf{W}_{\\mathcal{N}}^l \\mathbf{z}^l_t + \\mathbf{b}_{\\mathcal{N}}^l\\right) p^t_{\\mathcal{N}} +\n",
    "\\left( \\mathbf{W}_{\\mathcal{O}}^l \\mathbf{z}^l_t + \\mathbf{b}_{\\mathcal{O}}^l \\right) p^t_{\\mathcal{O}}.$\n",
    "\n",
    "\n",
    "In other words, the hidden representations $\\mathbf{z}^l_t$ are transformed using a convex combination of the four STNO class-specific affine transformations, weighted by the corresponding STNO class probabilities.\n",
    "\n",
    "![dicow_full](img/target_speaker_whisper_stno.drawio.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64edb329c931eea9",
   "metadata": {},
   "source": [
    "<a id='ts_asr'></a>\n",
    "## 2. Environment Setup <a id=\"setup\"></a>\n",
    "In this section, we will set up the environment for training and evaluating the DiCoW model on the Libri2Mix dataset. We will install the required dependencies, clone the repository.\n",
    "\n",
    "### Step 1: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "id": "935d8dac269925a9",
   "metadata": {},
   "source": [
    "with open(\"reqs_collab.txt\", \"w\") as f:\n",
    "    f.write(\n",
    "        \"\"\"accelerate>=0.33.0\n",
    "          datasets>=2.21.0\n",
    "          evaluate>=0.4.2\n",
    "          huggingface-hub==0.24.6\n",
    "          hydra-core==1.3.2\n",
    "          intervaltree==3.1.0\n",
    "          jiwer==2.5.2\n",
    "          kaldiio==2.18.0\n",
    "          lhotse==1.28.0\n",
    "          librosa==0.10.2.post1\n",
    "          meeteval==0.3.0\n",
    "          pandas==2.2.2\n",
    "          pyannote.core==5.0.0\n",
    "          pyannote.database==5.1.0\n",
    "          pyannote.metrics==3.2.1\n",
    "          PyYAML==6.0.2\n",
    "          transformers==4.41.2\n",
    "          wandb>=0.19.0\n",
    "          simplejson==3.20.1\n",
    "          \"\"\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7e2c2e40b67354b3",
   "metadata": {},
   "source": [
    "# Install required packages\n",
    "!pip install -q -r reqs_collab.txt\n",
    "# !pip install -q gdown\n",
    "!pip uninstall peft -y\n",
    "!pip uninstall tensorflow -y"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cc10c199c364f9c5",
   "metadata": {},
   "source": [
    "### Step 2: Clone the Repository\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "a03cf868e7990630",
   "metadata": {},
   "source": [
    "# Clone the DiCoW repository\n",
    "!git clone https://github.com/BUTSpeechFIT/TS-ASR-Whisper.git\n",
    "%cd TS-ASR-Whisper\n",
    "\n",
    "# Initialize and update submodules\n",
    "!git submodule init\n",
    "!git submodule update\n",
    "%cd .."
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "54d6af60297d2dd1",
   "metadata": {},
   "source": [
    "## 3. Data Preparation <a id=\"data\"></a>\n",
    "In this section, we will prepare the Libri2Mix dataset for training and evaluation. We will download the dataset, unzip it, and prepare the manifests for training and evaluation.\n",
    "\n",
    "#### 1. Prepare directories\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "22ffef360722427b",
   "metadata": {},
   "source": [
    "!mkdir -p data\n",
    "!mkdir -p data/libri2mix\n",
    "!mkdir -p data/manifests\n",
    "!mkdir -p data/libri2mix/train-100\n",
    "!mkdir -p data/libri2mix/dev\n",
    "!mkdir -p data/libri2mix/test"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "6d5c10cda51cbe49",
   "metadata": {},
   "source": [
    "#### 2. Download prepared Libri2Mix 100h clean dataset\n",
    "You can download the prepared Libri2Mix 100h clean dataset from the Google Drive link below.\n",
    "\n",
    "However, bandwidth limit could be exceeded and access could be denied. In that case, you can use the bash script below to download the dataset directly from the nextcloud server.\n",
    "\n",
    "The dataset is already preprocessed and ready for use. The dataset contains 100 hours of clean speech data, which is a subset of the original Libri2Mix dataset."
   ]
  },
  {
   "cell_type": "code",
   "id": "7bb11b0dbd338e2c",
   "metadata": {},
   "source": [
    "# import gdown\n",
    "#\n",
    "# gdown.download_folder(\"https://drive.google.com/drive/folders/1vZEroIOIa2H8JqAltGxFebBv_ukiKU4j?usp=sharing\", use_cookies=False,\n",
    "#                       quiet=True, output=\"data/libri2mix\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "%%bash\n",
    "# Cutsets\n",
    "curl -L -o data/libri2mix/libri2mix_mix_clean_sc_dev_cutset.jsonl.gz https://nextcloud.fit.vutbr.cz/s/MHLxjrd8XWPCieE/download\n",
    "curl -L -o data/libri2mix/libri2mix_clean_100_train_sc_cutset_30s.jsonl.gz https://nextcloud.fit.vutbr.cz/s/gyPBwcMM3Azqbpk/download\n",
    "curl -L -o data/libri2mix/libri2mix_mix_clean_sc_test_cutset.jsonl.gz https://nextcloud.fit.vutbr.cz/s/gdDMe2EKdAn4Kx5/download\n",
    "\n",
    "# Data\n",
    "curl -L -o data/libri2mix/train_mix_clean.tar.gz https://nextcloud.fit.vutbr.cz/s/oXkxkW59xDKLPgJ/download\n",
    "curl -L -o data/libri2mix/dev_mix_clean.tar.gz https://nextcloud.fit.vutbr.cz/s/DmiAicG2aWLeLqm/download\n",
    "curl -L -o data/libri2mix/test_mix_clean.tar.gz https://nextcloud.fit.vutbr.cz/s/WACLidXg78BgesB/download"
   ],
   "id": "d52dfb5570180316",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "609c487c91cedc7",
   "metadata": {},
   "source": [
    "#### 3. Unzip downloaded datasets"
   ]
  },
  {
   "cell_type": "code",
   "id": "22ec580fa230ffc",
   "metadata": {},
   "source": [
    "!tar -xzf data/libri2mix/train_mix_clean.tar.gz -C data/libri2mix/train-100\n",
    "!tar -xzf data/libri2mix/dev_mix_clean.tar.gz -C data/libri2mix/dev\n",
    "!tar -xzf data/libri2mix/test_mix_clean.tar.gz -C data/libri2mix/test"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "333a5713ebb66f30",
   "metadata": {},
   "source": [
    "#### 4. Fix paths in the dataset manifests"
   ]
  },
  {
   "cell_type": "code",
   "id": "d710c87717769ff3",
   "metadata": {},
   "source": [
    "from lhotse import load_manifest\n",
    "import os\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    for cutset, out in [\n",
    "        (\"data/libri2mix/libri2mix_clean_100_train_sc_cutset_30s.jsonl.gz\",\n",
    "         \"data/manifests/libri2mix_clean_100_train_sc_cutset_30s.jsonl.gz\"),\n",
    "        (\"data/libri2mix/libri2mix_mix_clean_sc_dev_cutset.jsonl.gz\",\n",
    "         \"data/manifests/libri2mix_mix_clean_sc_dev_cutset.jsonl.gz\"),\n",
    "        (\"data/libri2mix/libri2mix_mix_clean_sc_test_cutset.jsonl.gz\",\n",
    "         \"data/manifests/libri2mix_mix_clean_sc_test_cutset.jsonl.gz\")]:\n",
    "        cset = load_manifest(cutset)\n",
    "        for r in cset:\n",
    "            for src in r.recording.sources:\n",
    "                src.source = src.source.replace(\n",
    "                    'PATH_TO_BE_REPLACED',\n",
    "                    os.path.abspath('data/libri2mix'))\n",
    "            for s in r.supervisions:\n",
    "                s.alignment = None\n",
    "\n",
    "        cset.to_file(out)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "2c4969a03ae4b5a",
   "metadata": {},
   "source": [
    "#### 5. Check the dataset\n",
    "Below you can see characteristics of the training cutset. The dataset contains 100 hours of clean speech data, which is a subset of the original Libri2Mix dataset. The dataset is already preprocessed and ready for use."
   ]
  },
  {
   "cell_type": "code",
   "id": "f5800c645f5506aa",
   "metadata": {},
   "source": [
    "from lhotse import load_manifest\n",
    "\n",
    "train_cutset = load_manifest(\"data/manifests/libri2mix_clean_100_train_sc_cutset_30s.jsonl.gz\")\n",
    "train_cutset.describe()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Here you can see and hear example of data in the cutset. Each cut contains audio recording and supervisions for each speaker in the recording. The supervisions contain speaker name and transcription of the speech. Dataset was constructed from LibriSpeech by mixing two samples into one audio recording.",
   "id": "373ae38ea4460a2b"
  },
  {
   "cell_type": "code",
   "id": "6bffa6c4d91416dc",
   "metadata": {},
   "source": [
    "sample = train_cutset[0]\n",
    "print(f\"{sample.supervisions[0].speaker}: {sample.supervisions[0].text}\")\n",
    "print(f\"{sample.supervisions[1].speaker}: {sample.supervisions[1].text}\")\n",
    "sample.plot_audio()\n",
    "sample.play_audio()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### 6. Try to submit the same file to the DiCoW gradio app:\n",
    "You can try to submit the same file to the DiCoW gradio app and see how it performs. The app will use the pretrained DiCoW model to transcribe the audio and return the transcription for all speakers in the audio. You can also try to upload your own audio file and see how it performs.\n",
    "https://pccnect.fit.vutbr.cz/gradio-demo/"
   ],
   "id": "da364a439eb2b02c"
  },
  {
   "cell_type": "markdown",
   "id": "78084b5673b247ac",
   "metadata": {},
   "source": [
    "#### 7. Prepare small development set for quick testing\n",
    "Depending on how well you want to estimate the performance of your model, you can use the full development set or a smaller subset. Approximation on 128 cuts should be enough to get a rough estimate of the performance of your model. The full development set contains 3000 cuts.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "19c4050ada452286",
   "metadata": {},
   "source": [
    "from lhotse import load_manifest\n",
    "\n",
    "devset = load_manifest(\"data/manifests/libri2mix_mix_clean_sc_dev_cutset.jsonl.gz\")\n",
    "devset =devset.subset(first=128)\n",
    "devset.to_file(\"data/manifests/libri2mix_mix_clean_sc_dev_cutset_100.jsonl.gz\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "334546767fbc82d3",
   "metadata": {},
   "source": [
    "import os\n",
    "MANIFEST_DIR = os.path.abspath(\"data/manifests\")\n",
    "os.environ[\"MANIFEST_DIR\"] = MANIFEST_DIR\n",
    "os.environ[\"TRAIN_CUTSET\"] = f\"{MANIFEST_DIR}/libri2mix_clean_100_train_sc_cutset_30s.jsonl.gz\"\n",
    "os.environ[\"DEV_CUTSET\"] = f\"{MANIFEST_DIR}/libri2mix_mix_clean_sc_dev_cutset.jsonl.gz\"\n",
    "os.environ[\"TEST_CUTSET\"] = f\"{MANIFEST_DIR}/libri2mix_mix_clean_sc_test_cutset.jsonl.gz\"\n",
    "os.environ[\"TOYSET_CUTSET\"] = f\"{MANIFEST_DIR}/libri2mix_mix_clean_sc_dev_cutset_100.jsonl.gz\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "bc56dfe2c60bc368",
   "metadata": {},
   "source": [
    "## 4. Model training\n",
    "First let's try to check of the environment is set up correctly and if we can run the decoding script on a small dataset."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "%cd TS-ASR-Whisper",
   "id": "b43169b84590ab6e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "#### Step 1: Prepare the decoding configuration file"
   ],
   "id": "fdf0a796d11dd74f"
  },
  {
   "cell_type": "code",
   "id": "bd7a7500d851f35",
   "metadata": {},
   "source": [
    "with open(\"configs/decode/toyset_decoding.yaml\", \"w\") as f:\n",
    "    f.write(\n",
    "\"\"\"\n",
    "# @package _global_\n",
    "experiment: libri2mix_decode_both\n",
    "\n",
    "model:\n",
    "  whisper_model: \"openai/whisper-tiny\"\n",
    "data:\n",
    "  eval_cutsets: \"${oc.env:TOYSET_CUTSET}\"\n",
    "  train_cutsets: \"${oc.env:TOYSET_CUTSET}\"\n",
    "  dev_cutsets: \"${oc.env:TOYSET_CUTSET}\"\n",
    "  eval_text_norm: \"whisper_nsf\"\n",
    "training:\n",
    "  decode_only: true\n",
    "  bf16: false\n",
    "  bf16_full_eval: false\n",
    "  eval_metrics_list: [ \"tcp_wer\", \"cp_wer\"]\n",
    "  per_device_eval_batch_size: 16\n",
    "  dataloader_num_workers: 4\n",
    "  dataloader_prefetch_factor: 1\n",
    "  dataloader_pin_memory: true\n",
    "\"\"\"\n",
    "    )"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "3b90b916da08a6cb",
   "metadata": {},
   "source": [
    "##### Step 2: Export the environment variables for decoding"
   ]
  },
  {
   "cell_type": "code",
   "id": "1b72685706ed864d",
   "metadata": {},
   "source": [
    "import os\n",
    "os.environ[\"SRC_ROOT\"] = os.path.abspath(\".\")\n",
    "os.environ[\"WANDB_ANONYMOUS\"] = \"allow\"\n",
    "# os.environ[\"WANDB_ENTITY\"] = \"\"  # Set your Weights & Biases entity if needed\n",
    "os.environ[\"WANDB_PROJECT\"] = \"DiCoW_playground\"\n",
    "os.environ[\"WANDB_RUN_ID\"] = \"libri2mix_decode_both\"\n",
    "os.environ[\"HF_HOME\"] = \"hf_cache\"\n",
    "os.environ[\"PYTHONPATH\"] = f\"{os.environ['SRC_ROOT']}\"\n",
    "os.environ[\"EXPERIMENT_PATH\"] = f\"{os.environ['SRC_ROOT']}/exp/{os.environ.get('EXPERIMENT', '')}\"\n",
    "os.environ[\"LIBRI_TRAIN_CACHED_PATH\"] = \"\"\n",
    "os.environ[\"LIBRI_DEV_CACHED_PATH\"] = \"\"\n",
    "os.environ[\"AUDIO_PATH_PREFIX\"] = \"\"\n",
    "os.environ[\"AUDIO_PATH_PREFIX_REPLACEMENT\"] = \"\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Step 3: Run the decoding script",
   "id": "c390cc51fcc27b05"
  },
  {
   "cell_type": "code",
   "id": "a1e975bfec102486",
   "metadata": {},
   "source": "!python src/main.py +decode=toyset_decoding",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Step 4: Check the decoding results via meeteval\n",
    "You should see that randomly initialized model does not perform well. Let's fine-tune it a bit."
   ],
   "id": "a1bf25bcf4c80f7f"
  },
  {
   "cell_type": "code",
   "id": "8be916aea2135c33",
   "metadata": {},
   "source": [
    "import meeteval\n",
    "from meeteval.viz.visualize import AlignmentVisualization\n",
    "\n",
    "folder = r'exp/libri2mix_decode_both/test/0/wer/1919-142785-0014_3000-15664-0027'\n",
    "av = AlignmentVisualization(\n",
    "    meeteval.io.load(folder + '/ref.json'),\n",
    "    meeteval.io.load(folder + '/tcp_wer_hyp.json')\n",
    ")\n",
    "display(av)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### 5. DiCoW Training\n",
    "Create a configuration file for training the DiCoW-tiny model on the Libri2Mix dataset."
   ],
   "id": "a139b4545bd6741d"
  },
  {
   "cell_type": "code",
   "id": "ccf4b76a28d37366",
   "metadata": {},
   "source": [
    "with open(\"configs/train/tiny_l2mix.yaml\", \"w\") as f:\n",
    "    f.write(\n",
    "\"\"\"\n",
    "# @package _global_\n",
    "defaults:\n",
    "  - /train/icassp/table1_model_comparisons/base\n",
    "\n",
    "experiment: lsmix_tiny\n",
    "wandb:\n",
    "  project: jsalt25_dicow_challenge\n",
    "model:\n",
    "  whisper_model: openai/whisper-tiny\n",
    "  reinit_encoder_from: null\n",
    "data:\n",
    "  train_text_norm: \"whisper_nsf\"\n",
    "  use_timestamps: true\n",
    "  eval_cutsets: \"${oc.env:TEST_CUTSET}\"\n",
    "  train_cutsets: \"${oc.env:TRAIN_CUTSET}\"\n",
    "  dev_cutsets: \"${oc.env:TOYSET_CUTSET}\"\n",
    "  eval_text_norm: \"whisper_nsf\"\n",
    "\n",
    "training:\n",
    "  warmup_steps: 2000\n",
    "  remove_timestamps_from_ctc: true\n",
    "  overall_batch_size: 24\n",
    "  learning_rate: 1e-5\n",
    "  per_device_eval_batch_size: 16\n",
    "  bf16: false\n",
    "  bf16_full_eval: false\n",
    "  fp16: true\n",
    "  fp16_full_eval: true\n",
    "  eval_metrics_list: [ \"tcp_wer\", \"cp_wer\"]\n",
    "  eval_strategy: steps\n",
    "  save_strategy: steps\n",
    "  eval_steps: 200\n",
    "  save_steps: 200\n",
    "  use_amplifiers_only_n_epochs: 0\n",
    "\"\"\"\n",
    "    )"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Let's now train the DiCoW model on the Libri2Mix dataset. We will use the tiny Whisper model as a base and finetune it with DiCoW. Anytime you are satisfied with the results, you can stop the training and use the model for decoding.",
   "id": "80790785752a8242"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "!python src/main.py +train=tiny_l2mix",
   "id": "23dd94a18f917d0c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 5. Decoding & Evaluation\n",
    "Congrats you have trained the DiCoW model on the Libri2Mix dataset! üéâ\n",
    "\n",
    "Now let's decode the test set and evaluate the performance of the model."
   ],
   "id": "ced6b00bf8e45e1e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "with open(\"configs/decode/tiny_test.yaml\", \"w\") as f:\n",
    "    f.write(\n",
    "\"\"\"\n",
    "# @package _global_\n",
    "experiment: libri2mix_clean\n",
    "\n",
    "model:\n",
    "  whisper_model: \"openai/whisper-tiny\"\n",
    "  reinit_from: \"/content/TS-ASR-Whisper/exp/lsmix_tiny/checkpoint-600/model.safetensors\"\n",
    "data:\n",
    "  eval_cutsets: \"${oc.env:TEST_CUTSET}\"\n",
    "  train_cutsets: \"${oc.env:TOYSET_CUTSET}\"\n",
    "  dev_cutsets: \"${oc.env:TOYSET_CUTSET}\"\n",
    "  eval_text_norm: \"whisper_nsf\"\n",
    "training:\n",
    "  decode_only: true\n",
    "  bf16: false\n",
    "  bf16_full_eval: false\n",
    "  dataloader_num_workers: 4\n",
    "  dataloader_prefetch_factor: 1\n",
    "  dataloader_pin_memory: true\n",
    "  generation_max_length: 64\n",
    "  eval_metrics_list: [ \"tcp_wer\", \"cp_wer\"]\n",
    "  per_device_eval_batch_size: 64\n",
    "\"\"\"\n",
    "    )"
   ],
   "id": "e1bf6daac9b8343",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "!python src/main.py +decode=tiny_test",
   "id": "2ffc4b80fb32a8d9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import meeteval\n",
    "import glob\n",
    "hyps = []\n",
    "for hyp_file in glob.glob(\"/content/TS-ASR-Whisper/exp/libri2mix_clean/test/0/wer/*/tcp_wer_hyp.json\"):\n",
    "    hyps.append(meeteval.io.load(hyp_file))\n",
    "hyp = meeteval.io.SegLST.merge(*hyps)\n",
    "hyp.dump(f\"my_submission.json\")"
   ],
   "id": "db2804dd07709632",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 6. Generate Submission",
   "id": "f8bbf450e53ff61d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now you can collect all hypotheses and create a submission file. After creation of the submission file, you can upload it to the [EMMA Leaderboard](https://huggingface.co/spaces/BUT-FIT/EMMA_leaderboard).",
   "id": "df9c7d1c4e53f641"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import meeteval\n",
    "import glob\n",
    "hyps = []\n",
    "for hyp_file in glob.glob(\"/content/TS-ASR-Whisper/exp/libri2mix_clean/test/0/wer/*/tcp_wer_hyp.json\"):\n",
    "    hyps.append(meeteval.io.load(hyp_file))\n",
    "hyp = meeteval.io.SegLST.merge(*hyps)\n",
    "hyp.dump(f\"/content/my_submission.json\")"
   ],
   "id": "3ef4f46bb4395423",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Congrats! You have successfully trained and evaluated the DiCoW model on the Libri2Mix dataset. You can now submit your results to the EMMA leaderboard and compete for the prizes. Good luck! I am becoming thirsty already! üç∫üç∫üç∫",
   "id": "76f0ee12fb479f4a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "79bcdff130c4f413",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
